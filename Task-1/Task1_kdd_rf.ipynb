{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544a8f09",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96b686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    auc\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e96f233",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c19dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KDD Cup 1999 dataset (10% subset for speed)\n",
    "print(\"Loading KDD Cup 1999 dataset (10% subset)...\")\n",
    "data = fetch_kddcup99(percent10=True, shuffle=True, random_state=42)\n",
    "\n",
    "# Create DataFrame\n",
    "X_raw = pd.DataFrame(data.data)\n",
    "y_raw = pd.Series(data.target)\n",
    "\n",
    "print(f\"Dataset shape: {X_raw.shape}\")\n",
    "print(f\"Number of samples: {len(X_raw)}\")\n",
    "print(f\"Number of features: {X_raw.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4f5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign column names (KDD has 41 features)\n",
    "X_raw.columns = [f\"f{i}\" for i in range(X_raw.shape[1])]\n",
    "\n",
    "# Display first few rows\n",
    "print(\"First 5 rows of raw data:\")\n",
    "X_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592cf885",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb6ccc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode bytes to strings where needed (object dtype)\n",
    "for col in X_raw.select_dtypes([object]).columns:\n",
    "    X_raw[col] = X_raw[col].apply(lambda v: v.decode() if isinstance(v, bytes) else v)\n",
    "\n",
    "print(\"Data types after decoding:\")\n",
    "print(X_raw.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d16d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process target variable: decode and map to binary (normal -> 0, attack -> 1)\n",
    "y = y_raw.apply(lambda b: b.decode() if isinstance(b, bytes) else b)\n",
    "y_binary = (y != 'normal.').astype(int)\n",
    "\n",
    "# Check class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"Normal (0): {(y_binary == 0).sum()} ({(y_binary == 0).sum() / len(y_binary) * 100:.2f}%)\")\n",
    "print(f\"Attack (1): {(y_binary == 1).sum()} ({(y_binary == 1).sum() / len(y_binary) * 100:.2f}%)\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "y_binary.value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c'])\n",
    "plt.title('Class Distribution: Normal vs Attack', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Class (0=Normal, 1=Attack)', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc77ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify categorical columns in KDD\n",
    "# In KDD Cup 1999: f1=protocol_type, f2=service, f3=flag\n",
    "categorical_cols = ['f1', 'f2', 'f3']\n",
    "numeric_cols = [c for c in X_raw.columns if c not in categorical_cols]\n",
    "\n",
    "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"\\nCategorical features: {categorical_cols}\")\n",
    "\n",
    "# Show unique values in categorical columns\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}: {X_raw[col].nunique()} unique values\")\n",
    "    print(f\"Sample values: {X_raw[col].unique()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4551d9",
   "metadata": {},
   "source": [
    "## 4. Build Preprocessing and Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f71d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_cols),\n",
    "        (\"num\", StandardScaler(), numeric_cols),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Create full pipeline with RandomForest classifier\n",
    "pipeline = Pipeline([\n",
    "    (\"pre\", preprocessor),\n",
    "    (\"clf\", RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=20,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=4,\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"Pipeline created successfully!\")\n",
    "print(\"\\nPipeline steps:\")\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2814ca70",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets (80-20 split, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_raw, \n",
    "    y_binary, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_binary\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(f\"Normal: {(y_train == 0).sum()} | Attack: {(y_train == 1).sum()}\")\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(f\"Normal: {(y_test == 0).sum()} | Attack: {(y_test == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7278b6",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the pipeline\n",
    "print(\"Training Random Forest model...\\n\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"\\nModel training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cbff1",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b6180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Training accuracy\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Training Accuracy: {train_score:.4f}\")\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report (Key SOC Metrics)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT (Binary: Normal vs Attack)\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Normal', 'Attack'], digits=4))\n",
    "\n",
    "# ROC AUC Score\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"\\nROC AUC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873a8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Normal', 'Attack'],\n",
    "            yticklabels=['Normal', 'Attack'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Network Intrusion Detection', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate specific metrics from confusion matrix\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Breakdown:\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp} ← False alarms (normal classified as attack)\")\n",
    "print(f\"False Negatives (FN): {fn} ← Missed attacks (CRITICAL for SOC!)\")\n",
    "print(f\"True Positives (TP): {tp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='#3498db', lw=2, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='#95a5a6', lw=2, linestyle='--', label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "plt.title('ROC Curve - Network Intrusion Detection', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower right', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8bad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve (Important for imbalanced datasets)\n",
    "precision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, color='#e74c3c', lw=2, label=f'PR Curve (AUC = {pr_auc:.4f})')\n",
    "plt.xlabel('Recall (Sensitivity)', fontsize=12)\n",
    "plt.ylabel('Precision', fontsize=12)\n",
    "plt.title('Precision-Recall Curve - Critical for SOC Operations', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc='lower left', fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Precision-Recall AUC: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf7928",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances from Random Forest\n",
    "rf_model = pipeline.named_steps['clf']\n",
    "feature_importances = rf_model.feature_importances_\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "preprocessor_fitted = pipeline.named_steps['pre']\n",
    "cat_features = preprocessor_fitted.named_transformers_['cat'].get_feature_names_out(categorical_cols)\n",
    "all_feature_names = list(cat_features) + numeric_cols\n",
    "\n",
    "# Create DataFrame for feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': all_feature_names,\n",
    "    'importance': feature_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 20 Most Important Features:\")\n",
    "print(feature_importance_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d4bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top 20 feature importances\n",
    "top_n = 20\n",
    "top_features = feature_importance_df.head(top_n)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(top_n), top_features['importance'].values, color='#3498db')\n",
    "plt.yticks(range(top_n), top_features['feature'].values)\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.title(f'Top {top_n} Most Important Features - Random Forest', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a041141",
   "metadata": {},
   "source": [
    "## 9. Cross-Validation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6783d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 5-fold cross-validation on training set\n",
    "print(\"Performing 5-fold cross-validation...\\n\")\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c55bb",
   "metadata": {},
   "source": [
    "## 10. Save Model and Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e20e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete pipeline (preprocessing + model)\n",
    "model_filename = 'kdd_rf_pipeline.joblib'\n",
    "joblib.dump(pipeline, model_filename)\n",
    "\n",
    "print(f\"✓ Model pipeline saved to: {model_filename}\")\n",
    "print(f\"\\nModel file size: {joblib.os.path.getsize(model_filename) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Test loading the model\n",
    "loaded_pipeline = joblib.load(model_filename)\n",
    "test_prediction = loaded_pipeline.predict(X_test[:5])\n",
    "print(f\"\\n✓ Model loaded successfully!\")\n",
    "print(f\"Sample predictions from loaded model: {test_prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8e8d0",
   "metadata": {},
   "source": [
    "## 11. Model Summary and SOC Integration Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b449d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MODEL SUMMARY - NETWORK INTRUSION DETECTION\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset: KDD Cup 1999 (10% subset)\")\n",
    "print(f\"Total samples: {len(X_raw):,}\")\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"\\nModel: Random Forest Classifier\")\n",
    "print(f\"Number of trees: 200\")\n",
    "print(f\"Max depth: 20\")\n",
    "print(f\"\\nPerformance Metrics (Test Set):\")\n",
    "print(f\"  • Accuracy: {test_score:.4f}\")\n",
    "print(f\"  • ROC AUC: {roc_auc:.4f}\")\n",
    "print(f\"  • Precision-Recall AUC: {pr_auc:.4f}\")\n",
    "print(f\"\\nCritical SOC Metrics:\")\n",
    "print(f\"  • False Negatives (Missed Attacks): {fn}\")\n",
    "print(f\"  • False Positives (False Alarms): {fp}\")\n",
    "print(f\"\\nModel saved to: {model_filename}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nSOC/SIEM INTEGRATION NOTES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "1. Model Input: 41 network traffic features (protocol, service, connection stats)\n",
    "2. Model Output: Binary classification (0=Normal, 1=Attack) + probability score\n",
    "3. Alert Threshold: Use probability > 0.5 for high-confidence alerts\n",
    "4. For SIEM integration:\n",
    "   - Feed real-time network logs through preprocessing pipeline\n",
    "   - Generate alerts for predictions with class=1\n",
    "   - Include probability score in alert metadata for prioritization\n",
    "   - Monitor false positive rate and adjust threshold if needed\n",
    "5. Recommended retraining: Weekly with new labeled attack data\n",
    "6. Key features to monitor: Top 20 features from importance analysis\n",
    "\"\"\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8376682",
   "metadata": {},
   "source": [
    "## 12. Example: Making Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98254fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict on sample test instances\n",
    "sample_data = X_test.head(10)\n",
    "sample_true = y_test.head(10)\n",
    "\n",
    "sample_predictions = pipeline.predict(sample_data)\n",
    "sample_probabilities = pipeline.predict_proba(sample_data)[:, 1]\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'True_Label': ['Normal' if x == 0 else 'Attack' for x in sample_true],\n",
    "    'Predicted_Label': ['Normal' if x == 0 else 'Attack' for x in sample_predictions],\n",
    "    'Attack_Probability': sample_probabilities,\n",
    "    'Correct': sample_true.values == sample_predictions\n",
    "})\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(results_df)\n",
    "print(f\"\\nAccuracy on sample: {results_df['Correct'].sum() / len(results_df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d0f5e0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates a complete ML pipeline for network intrusion detection:\n",
    "\n",
    "✓ **Data Loading**: KDD Cup 1999 dataset with 494,021 samples\n",
    "\n",
    "✓ **Preprocessing**: OneHotEncoding for categorical features, StandardScaler for numeric features\n",
    "\n",
    "✓ **Model**: Random Forest with 200 trees (highly accurate and interpretable)\n",
    "\n",
    "✓ **Evaluation**: Comprehensive metrics including confusion matrix, ROC curve, precision-recall\n",
    "\n",
    "✓ **Explainability**: Feature importance analysis for understanding key attack indicators\n",
    "\n",
    "✓ **Production-Ready**: Saved pipeline for deployment, ready for SIEM integration\n",
    "\n",
    "### Key Takeaways for SOC Operations:\n",
    "- **High Recall** is critical to minimize missed attacks (false negatives)\n",
    "- **Precision** matters to reduce alert fatigue from false positives\n",
    "- **Feature importance** helps SOC analysts understand which network characteristics indicate threats\n",
    "- **Probability scores** enable alert prioritization and risk-based response"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
